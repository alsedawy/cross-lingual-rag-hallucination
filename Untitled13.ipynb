{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP49YPDj8iyEOrIHtD7Ea0p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alsedawy/cross-lingual-rag-hallucination/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjCITsFr349c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"IT_Q&A.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(content)\n",
        "\n",
        "print(\"File IT_Q&A.txt has been created successfully!\")\n",
        "# ==============================================================================\n",
        "# ONE-CELL FINAL EXPERIMENT: RETRIEVAL vs RAG (EN + AR) WITH PLOTS\n",
        "# ==============================================================================\n",
        "\n",
        "# ----------------------------\n",
        "# 0. Install dependencies\n",
        "# ----------------------------\n",
        "!pip install -q rank_bm25 sentence-transformers scikit-learn matplotlib seaborn transformers\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Imports & Models\n",
        "# ----------------------------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "\n",
        "EMBEDDING_MODEL = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "try:\n",
        "    GENERATOR = pipeline(\"text-generation\", model=\"gpt2\", tokenizer=\"gpt2\", device=-1)\n",
        "    USE_GPT = True\n",
        "except:\n",
        "    USE_GPT = False\n",
        "    GENERATOR = None\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Helper functions\n",
        "# ----------------------------\n",
        "def get_embedding(text):\n",
        "    return EMBEDDING_MODEL.encode([text])[0]\n",
        "\n",
        "def retrieve_bm25(query, qa_pairs, top_k=3):\n",
        "    corpus = [q + \" \" + a for q, a in qa_pairs]\n",
        "    tokenized = [c.split() for c in corpus]\n",
        "    bm25 = BM25Okapi(tokenized)\n",
        "    scores = bm25.get_scores(query.split())\n",
        "    return sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "def semantic_search(query, qa_pairs, top_k=3):\n",
        "    corpus = [q + \" \" + a for q, a in qa_pairs]\n",
        "    doc_emb = EMBEDDING_MODEL.encode(corpus)\n",
        "    q_emb = get_embedding(query)\n",
        "    sims = cosine_similarity([q_emb], doc_emb)[0]\n",
        "    return sorted(enumerate(sims), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "\n",
        "def rerank(query, qa_pairs, top_k=3):\n",
        "    bm = retrieve_bm25(query, qa_pairs, 6)\n",
        "    idxs = [i for i,_ in bm]\n",
        "    docs = [qa_pairs[i][0]+\" \"+qa_pairs[i][1] for i in idxs]\n",
        "    sims = cosine_similarity([get_embedding(query)], EMBEDDING_MODEL.encode(docs))[0]\n",
        "    return [idxs[i] for i in np.argsort(sims)[::-1][:top_k]]\n",
        "\n",
        "def rrf_fuse(lists, k=3, c=60):\n",
        "    scores = defaultdict(float)\n",
        "    for lst in lists:\n",
        "        for r,i in enumerate(lst):\n",
        "            scores[i]+=1/(c+r+1)\n",
        "    return [i for i,_ in sorted(scores.items(), key=lambda x:x[1], reverse=True)[:k]]\n",
        "\n",
        "def generate_answer(question, ctxs):\n",
        "    if not USE_GPT:\n",
        "        return \" \".join(ctxs[:2])\n",
        "    prompt = \"Answer using only the context:\\n\" + \"\\n\".join(ctxs) + f\"\\nQuestion:{question}\\nAnswer:\"\n",
        "    out = GENERATOR(prompt, max_length=80, temperature=0.05, return_full_text=False)\n",
        "    return out[0][\"generated_text\"]\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Metrics\n",
        "# ----------------------------\n",
        "def f1(pred, gold):\n",
        "    p=set(pred.lower().split())\n",
        "    g=set(gold.lower().split())\n",
        "\n",
        "    if not p or not g:\n",
        "        return 0.0\n",
        "\n",
        "    inter=len(p & g)\n",
        "    if inter==0:\n",
        "        return 0.0\n",
        "\n",
        "    pr=inter/len(p)\n",
        "    rc=inter/len(g)\n",
        "\n",
        "    if pr+rc==0:\n",
        "        return 0.0\n",
        "\n",
        "    return 2*pr*rc/(pr+rc)\n",
        "\n",
        "\n",
        "def hallucination(pred, ctxs):\n",
        "    p=set(pred.lower().split())\n",
        "    c=set(\" \".join(ctxs).lower().split())\n",
        "    return len([t for t in p if t not in c])/len(p) if p else 0\n",
        "\n",
        "def fidelity(pred, ctxs):\n",
        "    p=set(pred.lower().split())\n",
        "    c=set(\" \".join(ctxs).lower().split())\n",
        "    return len([t for t in p if t in c])/len(p) if p else 0\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Load Dataset\n",
        "# ----------------------------\n",
        "with open(\"IT_Q&A.txt\",\"r\",encoding=\"latin-1\") as f:\n",
        "    txt=f.read()\n",
        "\n",
        "qa_pairs=[]\n",
        "q,ans,read=None,[],False\n",
        "for l in [x.strip() for x in txt.split(\"\\n\") if x.strip()]:\n",
        "    if l.startswith(\"Q.\") and l.endswith(\"?\"):\n",
        "        if q and ans: qa_pairs.append((q,\" \".join(ans)))\n",
        "        q=l[2:]; ans=[]; read=False\n",
        "    elif l.startswith(\"A.\"):\n",
        "        read=True; ans.append(l[2:])\n",
        "    elif read:\n",
        "        ans.append(l)\n",
        "if q and ans: qa_pairs.append((q,\" \".join(ans)))\n",
        "\n",
        "print(\"Loaded QA:\",len(qa_pairs))\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Test sets\n",
        "# ----------------------------\n",
        "test_set_en=[\n",
        "(\"What is cache memory?\",\"Cache memory is a small, fast memory buffer\"),\n",
        "(\"What is an API?\",\"An API is a set of instructions\"),\n",
        "(\"What does a technical support engineer do?\",\"The work of a technical support engineer\"),\n",
        "]\n",
        "\n",
        "test_set_ar=[\n",
        "(\"ما هي الذاكرة المؤقتة؟\",\"Cache memory is a small, fast memory buffer\"),\n",
        "(\"ما هو API؟\",\"An API is a set of instructions\"),\n",
        "(\"ما وظيفة مهندس الدعم الفني؟\",\"The work of a technical support engineer\"),\n",
        "]\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Evaluation\n",
        "# ----------------------------\n",
        "def run_eval(test_set, mode):\n",
        "    res=[]\n",
        "    for qg,ag in test_set:\n",
        "        if mode==\"baseline\":\n",
        "            idxs=[i for i,_ in retrieve_bm25(qg,qa_pairs)]\n",
        "        elif mode==\"semantic\":\n",
        "            idxs=[i for i,_ in semantic_search(qg,qa_pairs)]\n",
        "        elif mode==\"rerank\":\n",
        "            idxs=rerank(qg,qa_pairs)\n",
        "        elif mode==\"fusion\":\n",
        "            idxs=rrf_fuse([\n",
        "                [i for i,_ in retrieve_bm25(qg,qa_pairs)],\n",
        "                [i for i,_ in semantic_search(qg,qa_pairs)]\n",
        "            ])\n",
        "        ctxs=[qa_pairs[i][1] for i in idxs]\n",
        "        pred=generate_answer(qg,ctxs)\n",
        "        res.append([f1(pred,ag),hallucination(pred,ctxs),fidelity(pred,ctxs)])\n",
        "    return np.mean(res,axis=0)\n",
        "\n",
        "methods=[\"baseline\",\"semantic\",\"rerank\",\"fusion\"]\n",
        "\n",
        "def eval_set(name,test):\n",
        "    data={m:run_eval(test,m) for m in methods}\n",
        "    df=pd.DataFrame(data,index=[\"F1\",\"Hallucination\",\"Fidelity\"]).T\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(df)\n",
        "    return df\n",
        "\n",
        "df_en=eval_set(\"English Test Set\",test_set_en)\n",
        "df_ar=eval_set(\"Arabic Test Set\",test_set_ar)\n",
        "\n",
        "# ----------------------------\n",
        "# 7. Plots\n",
        "# ----------------------------\n",
        "def plot(df,title):\n",
        "    df.plot(kind=\"bar\",figsize=(10,4))\n",
        "    plt.title(title)\n",
        "    plt.ylim(0,1)\n",
        "    plt.show()\n",
        "\n",
        "plot(df_en,\"English – Retrieval vs RAG\")\n",
        "plot(df_ar,\"Arabic – Retrieval vs RAG\")\n"
      ],
      "metadata": {
        "id": "J9X6GioyLx8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}